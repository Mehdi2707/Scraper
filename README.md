# ğŸ¤– Scraper Multi-Sites

Ce projet est une solution de surveillance automatisÃ©e conÃ§ue pour scruter plusieurs sites web Ã  la recherche de changements et envoyer des notifications par e-mail lorsqu'une alerte enregistrÃ©e en base de donnÃ©es est dÃ©clenchÃ©e. Il est construit sur une architecture modulaire, ce qui facilite l'ajout de nouveaux sites de scraping.

## ğŸš€ FonctionnalitÃ©s ClÃ©s

* **Scraping Multi-Site :** Prise en charge initiale de **Ticketmaster** (surveillance de billets/catÃ©gories), **LevelsAuto** (surveillance de nouveaux vÃ©hicules) et de **Site e-commerce** (surveillance de la disponibilitÃ©).
* **Architecture Modulaire :** Les logiques de scraping sont isolÃ©es dans des fichiers dÃ©diÃ©s (`utils/'specificScraper'`), permettant une extensibilitÃ© facile.
* **Surveillance Permanente :** Le programme tourne en boucle, exÃ©cutant les alertes de la base de donnÃ©es en continue.
* **DÃ©tection Intelligente :**
    * **Ticketmaster :** DÃ©tection de la disponibilitÃ© d'une catÃ©gorie spÃ©cifique ou gÃ©nÃ©rique (toutes catÃ©gories).
    * **LevelsAuto :** DÃ©tection de **nouveaux produits** par comparaison de contenu HTML stockÃ© en DB, garantissant que seules les annonces rÃ©ellement nouvelles dÃ©clenchent une notification.
    * **Site e-commerce :** DÃ©tection de la disponibilitÃ© d'un produit.
* **SystÃ¨me d'Alertes CentralisÃ© :** Les alertes sont gÃ©rÃ©es via une base de donnÃ©es (accÃ¨s via `database.js`).
* **Notifications par E-mail :** Envoi d'e-mails pour chaque Ã©vÃ©nement dÃ©tectÃ©.
* **Anti-DÃ©tection (Stealth) :** Utilisation de `puppeteer-extra` et du plugin `puppeteer-extra-plugin-stealth` pour minimiser le risque de blocage.

---

## ğŸ› ï¸ Installation

### PrÃ©requis

* **Node.js** (v18+)
* **Une base de donnÃ©es** (MySQL, MariaDB, etc.)
* **Un compte e-mail** et ses identifiants SMTP (pour l'envoi des notifications).

### Ã‰tapes

1.  **Cloner le dÃ©pÃ´t** :
    ```bash
    git clone https://github.com/Mehdi2707/Scraper.git
    cd Scraper
    ```

2.  **Installer les dÃ©pendances** :
    ```bash
    npm install
    ```

3.  **Configuration de l'environnement (`.env`)** :

    Renommer le fichier `.env.example` en `.env` et configurez les accÃ¨s Ã  la DB et au service d'e-mail.

4.  **Configuration de la Base de DonnÃ©es** :

    Assurez-vous que votre table `alerts` contient au moins les colonnes suivantes :

    | Colonne | Type | RÃ´le |
    | :--- | :--- | :--- |
    | **`id`** | INT (PK) | Identifiant de l'alerte. |
    | **`link`** | VARCHAR | URL cible du scraping. |
    | **`email`** | VARCHAR | E-mail du destinataire. |
    | **`categorie`** | VARCHAR | CatÃ©gorie/mot-clÃ© spÃ©cifique. |
    | **`is_closed`** | TINYINT | `1` si l'alerte est fermÃ©e. |
    | **`is_accessible`**| TINYINT | **Discriminateur de Scraper :** `0` pour Ticketmaster, `1` pour LevelsAuto. |
    | **`html`** | LONGTEXT | Stocke le HTML pour comparaison (utilisÃ© par LevelsAuto). |
    | **`close_alert`** | TINYINT | Indique si l'alerte doit Ãªtre fermÃ©e aprÃ¨s la premiÃ¨re notification. |

---

## âš™ï¸ Utilisation

Pour lancer le service de scraping en boucle :

```bash
node start-scrap.js
```

Vous pouvez Ã©galement lancer le service en arriÃ¨re plan (fichier de log Ã  la racine du projet):

```bash
node start-scrap.js >> scraper.log 2>&1 &
```

Puis l'arrÃªter avec le numÃ©ro du processus :

```bash
kill -9 `[NÂ° Process]`
```